p8105\_hw3\_ww2579
================
Wenzhao Wu
10/10/2020

``` r
library(tidyverse)
```

    ## -- Attaching packages -------------------------- tidyverse 1.3.0 --

    ## v ggplot2 3.3.2     v purrr   0.3.4
    ## v tibble  3.0.3     v dplyr   1.0.2
    ## v tidyr   1.1.2     v stringr 1.4.0
    ## v readr   1.3.1     v forcats 0.5.0

    ## -- Conflicts ----------------------------- tidyverse_conflicts() --
    ## x dplyr::filter() masks stats::filter()
    ## x dplyr::lag()    masks stats::lag()

``` r
library(p8105.datasets)
library(readxl)
data("ny_noaa")

knitr::opts_chunk$set(
  fig.width = 12,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

``` r
data("instacart")
```

This dataset contains 1384617 rows and `rcol(instacart)` columns.

Observations are the level of items in orders by user. There are user /
order variables – user ID, order ID, order day, and order hour. There
are also item variables – name, aisle, department, and some numeric
codes.

How many aisles, and which are most item from?

``` r
instacart %>%
  count(aisle) %>%
  arrange(desc(n))
```

    ## # A tibble: 134 x 2
    ##    aisle                              n
    ##    <chr>                          <int>
    ##  1 fresh vegetables              150609
    ##  2 fresh fruits                  150473
    ##  3 packaged vegetables fruits     78493
    ##  4 yogurt                         55240
    ##  5 packaged cheese                41699
    ##  6 water seltzer sparkling water  36617
    ##  7 milk                           32644
    ##  8 chips pretzels                 31269
    ##  9 soy lactosefree                26240
    ## 10 bread                          23635
    ## # ... with 124 more rows

Let’s make a plot

``` r
instacart %>%
  count(aisle) %>%
  filter(n > 1000) %>%  
  mutate(
    aisle = factor(aisle),
    aisle = fct_reorder(aisle, n)
  ) %>%
  ggplot(aes(x = aisle, y = n)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

<img src="p8105_hw3_ww2579_files/figure-gfm/unnamed-chunk-3-1.png" width="90%" />

Let’s make a table\!

``` r
instacart %>%
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle) %>%
  count(product_name) %>%
  mutate(rank = min_rank(desc(n))) %>%
  filter(rank < 4) %>%
  arrange(aisle, rank) %>%
  knitr::kable()
```

| aisle                      | product\_name                                 |    n | rank |
| :------------------------- | :-------------------------------------------- | ---: | ---: |
| baking ingredients         | Light Brown Sugar                             |  499 |    1 |
| baking ingredients         | Pure Baking Soda                              |  387 |    2 |
| baking ingredients         | Cane Sugar                                    |  336 |    3 |
| dog food care              | Snack Sticks Chicken & Rice Recipe Dog Treats |   30 |    1 |
| dog food care              | Organix Chicken & Brown Rice Recipe           |   28 |    2 |
| dog food care              | Small Dog Biscuits                            |   26 |    3 |
| packaged vegetables fruits | Organic Baby Spinach                          | 9784 |    1 |
| packaged vegetables fruits | Organic Raspberries                           | 5546 |    2 |
| packaged vegetables fruits | Organic Blueberries                           | 4966 |    3 |

Apples vs ice cream.

``` r
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  )
```

    ## `summarise()` regrouping output by 'product_name' (override with `.groups` argument)

    ## # A tibble: 2 x 8
    ## # Groups:   product_name [2]
    ##   product_name       `0`   `1`   `2`   `3`   `4`   `5`   `6`
    ##   <chr>            <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>
    ## 1 Coffee Ice Cream  13.8  14.3  15.4  15.3  15.2  12.3  13.8
    ## 2 Pink Lady Apples  13.4  11.4  11.7  14.2  11.6  12.8  11.9

## Problem 2

Load, tidy, and wrangle the data.

``` r
tidied_df = 
  read_csv("./data/accel_data.csv") %>%
  janitor::clean_names() %>%
  pivot_longer(
    activity_1:activity_1440,
    names_to = "activity_min",
    values_to = "activity_counts",
    names_prefix = "activity_") %>%
  mutate(
    dow = case_when(
      day %in% c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday") ~ "weekday",
      day %in% c("Saturday", "Sunday") ~ "weekend"),
    day = factor(day),
    dow = factor(dow),
    activity_min = as.numeric(activity_min))
```

    ## Parsed with column specification:
    ## cols(
    ##   .default = col_double(),
    ##   day = col_character()
    ## )

    ## See spec(...) for full column specifications.

``` r
tidied_df
```

    ## # A tibble: 50,400 x 6
    ##     week day_id day    activity_min activity_counts dow    
    ##    <dbl>  <dbl> <fct>         <dbl>           <dbl> <fct>  
    ##  1     1      1 Friday            1            88.4 weekday
    ##  2     1      1 Friday            2            82.2 weekday
    ##  3     1      1 Friday            3            64.4 weekday
    ##  4     1      1 Friday            4            70.0 weekday
    ##  5     1      1 Friday            5            75.0 weekday
    ##  6     1      1 Friday            6            66.3 weekday
    ##  7     1      1 Friday            7            53.8 weekday
    ##  8     1      1 Friday            8            47.8 weekday
    ##  9     1      1 Friday            9            55.5 weekday
    ## 10     1      1 Friday           10            43.0 weekday
    ## # ... with 50,390 more rows

This dataset contains 50400 rows and 6 columns.

It collects data of activity counts for each minute of a 24-hour day for
35 days (i.e. 5 weeks), with a total number of observations of 50400.

There are variables activity\_counts, activity\_min, day, day\_id, dow,
week included.

Data summary.

``` r
day_activity = 
  tidied_df %>%
  group_by(week, day, day_id) %>%
  summarise(activity_per_day = sum(activity_counts)) %>%
  knitr::kable()
```

    ## `summarise()` regrouping output by 'week', 'day' (override with `.groups` argument)

``` r
day_activity
```

| week | day       | day\_id | activity\_per\_day |
| ---: | :-------- | ------: | -----------------: |
|    1 | Friday    |       1 |          480542.62 |
|    1 | Monday    |       2 |           78828.07 |
|    1 | Saturday  |       3 |          376254.00 |
|    1 | Sunday    |       4 |          631105.00 |
|    1 | Thursday  |       5 |          355923.64 |
|    1 | Tuesday   |       6 |          307094.24 |
|    1 | Wednesday |       7 |          340115.01 |
|    2 | Friday    |       8 |          568839.00 |
|    2 | Monday    |       9 |          295431.00 |
|    2 | Saturday  |      10 |          607175.00 |
|    2 | Sunday    |      11 |          422018.00 |
|    2 | Thursday  |      12 |          474048.00 |
|    2 | Tuesday   |      13 |          423245.00 |
|    2 | Wednesday |      14 |          440962.00 |
|    3 | Friday    |      15 |          467420.00 |
|    3 | Monday    |      16 |          685910.00 |
|    3 | Saturday  |      17 |          382928.00 |
|    3 | Sunday    |      18 |          467052.00 |
|    3 | Thursday  |      19 |          371230.00 |
|    3 | Tuesday   |      20 |          381507.00 |
|    3 | Wednesday |      21 |          468869.00 |
|    4 | Friday    |      22 |          154049.00 |
|    4 | Monday    |      23 |          409450.00 |
|    4 | Saturday  |      24 |            1440.00 |
|    4 | Sunday    |      25 |          260617.00 |
|    4 | Thursday  |      26 |          340291.00 |
|    4 | Tuesday   |      27 |          319568.00 |
|    4 | Wednesday |      28 |          434460.00 |
|    5 | Friday    |      29 |          620860.00 |
|    5 | Monday    |      30 |          389080.00 |
|    5 | Saturday  |      31 |            1440.00 |
|    5 | Sunday    |      32 |          138421.00 |
|    5 | Thursday  |      33 |          549658.00 |
|    5 | Tuesday   |      34 |          367824.00 |
|    5 | Wednesday |      35 |          445366.00 |

After aggregating across minutes for each day, there are total
observations. One standing-out feature is that on Saturdays of week 4
and 5, the activity counts are both 1440 per day.

Make a single-panel plot.

``` r
plot_accel = 
  tidied_df %>%
  ggplot(aes(x = activity_min, y = activity_counts, group = day_id, color = day_id),alpha = 0.5) +
  geom_line() +
  scale_y_continuous(
    trans = "sqrt") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))

plot_accel
```

<img src="p8105_hw3_ww2579_files/figure-gfm/unnamed-chunk-8-1.png" width="90%" />

## Problem 3

``` r
ny_noaa = ny_noaa %>%
  separate(date, into = c("year", "month", "day"), convert = TRUE) %>%
  mutate(month = factor(month),
         tmax = as.numeric(tmax),
         tmin = as.numeric(tmin))
ny_noaa
```

    ## # A tibble: 2,595,176 x 9
    ##    id           year month   day  prcp  snow  snwd  tmax  tmin
    ##    <chr>       <int> <fct> <int> <int> <int> <int> <dbl> <dbl>
    ##  1 US1NYAB0001  2007 11        1    NA    NA    NA    NA    NA
    ##  2 US1NYAB0001  2007 11        2    NA    NA    NA    NA    NA
    ##  3 US1NYAB0001  2007 11        3    NA    NA    NA    NA    NA
    ##  4 US1NYAB0001  2007 11        4    NA    NA    NA    NA    NA
    ##  5 US1NYAB0001  2007 11        5    NA    NA    NA    NA    NA
    ##  6 US1NYAB0001  2007 11        6    NA    NA    NA    NA    NA
    ##  7 US1NYAB0001  2007 11        7    NA    NA    NA    NA    NA
    ##  8 US1NYAB0001  2007 11        8    NA    NA    NA    NA    NA
    ##  9 US1NYAB0001  2007 11        9    NA    NA    NA    NA    NA
    ## 10 US1NYAB0001  2007 11       10    NA    NA    NA    NA    NA
    ## # ... with 2,595,166 more rows

The most commonly observed values for snowfall.

``` r
ny_noaa %>%
  group_by(snow) %>%
  mutate(N = n()) %>%
  ungroup() %>%
  filter(N == max(N))
```

    ## # A tibble: 2,008,508 x 10
    ##    id           year month   day  prcp  snow  snwd  tmax  tmin       N
    ##    <chr>       <int> <fct> <int> <int> <int> <int> <dbl> <dbl>   <int>
    ##  1 US1NYAB0001  2007 11       18     0     0    NA    NA    NA 2008508
    ##  2 US1NYAB0001  2007 11       19     0     0     0    NA    NA 2008508
    ##  3 US1NYAB0001  2007 11       20    25     0     0    NA    NA 2008508
    ##  4 US1NYAB0001  2007 11       21    71     0     0    NA    NA 2008508
    ##  5 US1NYAB0001  2007 11       22     8     0    NA    NA    NA 2008508
    ##  6 US1NYAB0001  2007 11       23     8     0    NA    NA    NA 2008508
    ##  7 US1NYAB0001  2007 11       25     0     0    NA    NA    NA 2008508
    ##  8 US1NYAB0001  2007 11       27   150     0     0    NA    NA 2008508
    ##  9 US1NYAB0001  2007 11       29     0     0     0    NA    NA 2008508
    ## 10 US1NYAB0001  2007 11       30     0     0     0    NA    NA 2008508
    ## # ... with 2,008,498 more rows

The most commonly observed value is “0” as the code chunk shows above.

Jan vs July.

``` r
jj_plot = ny_noaa %>%
  filter(month %in% c("1","7")) %>%
  group_by(month,year,id) %>%
  summarize(
    mean_tmax = (mean(tmax,na.rm = TRUE))) %>%
  ggplot(aes(x = year, y = mean_tmax, group = id, color = id)) +
  geom_point() +
  geom_smooth(se = F) +
  facet_grid(~month) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),
        legend.position = "none")
```

    ## `summarise()` regrouping output by 'month', 'year' (override with `.groups` argument)

``` r
jj_plot
```

    ## `geom_smooth()` using method = 'loess' and formula 'y ~ x'

    ## Warning: Removed 5970 rows containing non-finite values (stat_smooth).

    ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric =
    ## parametric, : span too small. fewer data values than degrees of freedom.

    ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric =
    ## parametric, : pseudoinverse used at 1986

    ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric =
    ## parametric, : neighborhood radius 1.01

    ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric =
    ## parametric, : reciprocal condition number 0

    ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric =
    ## parametric, : There are other near singularities as well. 1.0201

    ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric =
    ## parametric, : Chernobyl! trL>n 6
    
    ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric =
    ## parametric, : Chernobyl! trL>n 6

    ## Warning in sqrt(sum.squares/one.delta): NaNs produced

    ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric =
    ## parametric, : span too small. fewer data values than degrees of freedom.

    ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric =
    ## parametric, : at 2009

    ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric =
    ## parametric, : radius 2.5e-005

    ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric =
    ## parametric, : all data on boundary of neighborhood. make span bigger

    ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric =
    ## parametric, : pseudoinverse used at 2009

    ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric =
    ## parametric, : neighborhood radius 0.005

    ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric =
    ## parametric, : reciprocal condition number 1

    ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric =
    ## parametric, : at 2010

    ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric =
    ## parametric, : radius 2.5e-005

    ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric =
    ## parametric, : all data on boundary of neighborhood. make span bigger

    ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric =
    ## parametric, : There are other near singularities as well. 2.5e-005

    ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric =
    ## parametric, : zero-width neighborhood. make span bigger
    
    ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric =
    ## parametric, : zero-width neighborhood. make span bigger

    ## Warning: Computation failed in `stat_smooth()`:
    ## NA/NaN/Inf in foreign function call (arg 5)

    ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric =
    ## parametric, : span too small. fewer data values than degrees of freedom.

    ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric =
    ## parametric, : pseudoinverse used at 1986

    ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric =
    ## parametric, : neighborhood radius 1.01

    ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric =
    ## parametric, : reciprocal condition number 0

    ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric =
    ## parametric, : There are other near singularities as well. 1.0201

    ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric =
    ## parametric, : span too small. fewer data values than degrees of freedom.

    ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric =
    ## parametric, : pseudoinverse used at 1981

    ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric =
    ## parametric, : neighborhood radius 2.025

    ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric =
    ## parametric, : reciprocal condition number 0

    ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric =
    ## parametric, : There are other near singularities as well. 9.1506

    ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric =
    ## parametric, : span too small. fewer data values than degrees of freedom.

    ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric =
    ## parametric, : at 2009

    ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric =
    ## parametric, : radius 2.5e-005

    ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric =
    ## parametric, : all data on boundary of neighborhood. make span bigger

    ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric =
    ## parametric, : pseudoinverse used at 2009

    ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric =
    ## parametric, : neighborhood radius 0.005

    ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric =
    ## parametric, : reciprocal condition number 1

    ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric =
    ## parametric, : at 2010

    ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric =
    ## parametric, : radius 2.5e-005

    ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric =
    ## parametric, : all data on boundary of neighborhood. make span bigger

    ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric =
    ## parametric, : There are other near singularities as well. 2.5e-005

    ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric =
    ## parametric, : zero-width neighborhood. make span bigger
    
    ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric =
    ## parametric, : zero-width neighborhood. make span bigger

    ## Warning: Computation failed in `stat_smooth()`:
    ## NA/NaN/Inf in foreign function call (arg 5)

    ## Warning: Removed 5970 rows containing missing values (geom_point).

<img src="p8105_hw3_ww2579_files/figure-gfm/unnamed-chunk-11-1.png" width="90%" />
